{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Approach:\n",
    "* Instead of the driver task creating bootstrap samples, that funcitonality is delegated to \n",
    "executor tasks by using Spark broadcast() function.  This reduces run time by avoiding recreating the original data set\n",
    "for every sample in an executor.\n",
    "* Each parallel task consists of a `sample_id` and `sample_seed`.  The `sample_seed` value is used by the executor\n",
    "task to create a bootstrap sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>5</td><td>application_1533220140196_0007</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-3-52.ec2.internal:20888/proxy/application_1533220140196_0007/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-3-59.ec2.internal:8042/node/containerlogs/container_1533220140196_0007_01_000001/jovyan\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import RDD\n",
    "import pickle\n",
    "import base64\n",
    "from pyspark import SparkConf, HiveContext\n",
    "from pyspark.sql import  Row\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import socket\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    .config('spark.shuffle.compress ','false')\\\n",
    "    .config('spark.shuffle.spill.compress','false')\\\n",
    "    .config('spark.task.maxFailures','8')\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"8g\")\\\n",
    "    .config('spark.executor.memory','16g')\\\n",
    "    .config('spark.executor.instances',str(NUMBER_EXECUTORS))\\\n",
    "    .config('spark.executor.cores',str(NUMBER_CORES))\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_ROWS = 4000000\n",
    "\n",
    "NUMBER_EXECUTORS = 32\n",
    "NUMBER_CORES = 2\n",
    "#NUMBER_PARTITIONS = NUMBER_EXECUTORS*NUMBER_CORES\n",
    "\n",
    "NUMBER_OF_SAMPLES = 1000 #SAMPLE_SETS * SAMPLE_SET_REPLICATIONS\n",
    "\n",
    "MAX_RANDOM_SEED = int(1e5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "spark.stop()\n",
    "spark = spark.builder\\\n",
    "    .appName('bootstrap redesign - spark2')\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\")\\\n",
    "    .config('spark.executor.memory','1g')\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()\n",
    "    #.config('spark.executor.heartbeatInterval','30s')\\\n",
    "    \n",
    "sc = spark.sparkContext\n",
    "hiveContext = HiveContext(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'2.3.1'"
     ]
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'application_1533220140196_0007'"
     ]
    }
   ],
   "source": [
    "sc.applicationId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f1ca27b5f98>"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(21)\n",
    "\n",
    "col1 = np.random.exponential(1,size=NUMBER_OF_ROWS)\n",
    "col2 = np.random.normal(0,1,NUMBER_OF_ROWS)\n",
    "col3 = np.random.randint(1,10,NUMBER_OF_ROWS)\n",
    "category = np.random.choice(['a','b','c','d','e'],NUMBER_OF_ROWS)\n",
    "orig_df = pd.DataFrame(dict(category=category,col1=col1,col2=col2,col3=col3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000000, 4)\n",
      "  category      col1      col2  col3\n",
      "0        b  0.049952  0.353572     1\n",
      "1        a  0.341237  1.814845     3\n",
      "2        b  1.276423  1.002329     4\n",
      "3        b  0.021853  1.184573     4\n",
      "4        b  0.230575  2.445508     5\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "a    801266\n",
      "c    800493\n",
      "b    799794\n",
      "d    799518\n",
      "e    798929\n",
      "Name: category, dtype: int64"
     ]
    }
   ],
   "source": [
    "print(orig_df.shape)\n",
    "print(orig_df.head(5))\n",
    "print(type(orig_df))\n",
    "orig_df.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.broadcast.Broadcast object at 0x7f1ca27b6a58>"
     ]
    }
   ],
   "source": [
    "# broadcast raw data to executors \n",
    "sc.broadcast(orig_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap core computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Bootstrap specific Exceptions\n",
    "class BootstrapError(Exception):\n",
    "    \"\"\"Custom excpetion for bootstrap analysis\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Common Calculation fucntion\n",
    "#\n",
    "def calculateSampleStats(df,col):\n",
    "    stat_mean = df[col].mean()\n",
    "    stat_min = df[col].min()\n",
    "    stat_max= df[col].max()\n",
    "    stat_50th = df[col].quantile(0.5)\n",
    "    \n",
    "    return stat_min, stat_mean, stat_max, stat_50th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# function to return summary of sample processing\n",
    "#   Returns a single row of results for the sample\n",
    "#\n",
    "def processASampleReturnSummary(sample_run):\n",
    "    # iterator: Python iterator for each record in a sample\n",
    "    \n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # retrieve sample id and seed for sampling\n",
    "    sample_id = sample_run['sample_id']\n",
    "    sample_seed = sample_run['sample_seed']\n",
    "\n",
    "    # create bootstrap sample using the specified sample_seed value\n",
    "    sample_df = orig_df.sample(n=orig_df.shape[0],replace=True,random_state=sample_seed)\n",
    "\n",
    "    sample_df['col2'] = sample_df['col2'] + 10*sample_id\n",
    "\n",
    "    result_stats = dict()\n",
    "\n",
    "    for c in ['col2','col1']:\n",
    "        stats = calculateSampleStats(sample_df,c)\n",
    "        stats_to_return = ['min','mean','max','50th']\n",
    "\n",
    "        result_stats.update(dict(zip([c + '_' + stat for stat in stats_to_return],\n",
    "                            [float(x) for x in stats])))\n",
    "\n",
    "    # caculate run-time performance measures\n",
    "    end_time = datetime.datetime.now()\n",
    "\n",
    "    elapsed_time_str = '{}'.format(end_time - start_time)\n",
    "\n",
    "    start_str = '{}'.format(start_time)\n",
    "    end_str = '{}'.format(end_time)\n",
    "\n",
    "    print('>>>>>>Pid: {:d}, completed processing sample_id {:d} at {}'\\\n",
    "          .format(os.getpid(),sample_id,datetime.datetime.now()))\n",
    "\n",
    "     # return results of bootstrap analysis\n",
    "    return dict(sample_id=sample_id, sample_seed=sample_seed, \n",
    "                                  shape=str(sample_df.shape),\n",
    "                      worker_hostname = socket.gethostname(),\n",
    "                      worker_pid = os.getpid(),\n",
    "                      time_start=start_str, time_end=end_str,\n",
    "                      time_elapsed=elapsed_time_str ,    \n",
    "                **result_stats)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and analyze bootstrap samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis for 1,000 samples"
     ]
    }
   ],
   "source": [
    "print('Starting analysis for {:,d} samples'\\\n",
    "     .format(NUMBER_OF_SAMPLES))\n",
    "bootstrap_start = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create sample ids and seeds to be use in boostrap sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sample_id  sample_seed\n",
      "0          1        72031\n",
      "1          2        27978\n",
      "2          3        55639\n",
      "3          4        51955\n",
      "4          5        52145\n",
      "5          6         3011\n",
      "6          7        83607\n",
      "7          8        68952\n",
      "8          9        90269\n",
      "9         10        69234"
     ]
    }
   ],
   "source": [
    "np.random.seed(13)  # make repeatable\n",
    "\n",
    "sample_seeds = pd.DataFrame(dict(sample_id=np.array(range(NUMBER_OF_SAMPLES))+1,\n",
    "                                sample_seed= np.random.choice(range(MAX_RANDOM_SEED),\n",
    "                                                              size=NUMBER_OF_SAMPLES,\n",
    "                                                             replace=False)))\n",
    "sample_seeds.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sample_id  sample_seed\n",
      "990        991        41440\n",
      "991        992        92587\n",
      "992        993          166\n",
      "993        994        24458\n",
      "994        995        74793\n",
      "995        996        21813\n",
      "996        997        62437\n",
      "997        998        32343\n",
      "998        999        81160\n",
      "999       1000        53014"
     ]
    }
   ],
   "source": [
    "sample_seeds.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the parallel tasks to create sample and compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_rdd partitions: 100\n",
      "completed creating pandas dataframe creation time: 0:04:57.137409\n",
      "(1000, 16)\n",
      "shape of bootstrap_results is (1000, 16)"
     ]
    }
   ],
   "source": [
    "# create RDD to contain sample_seed to create each bootstrap sample in the executors\n",
    "sample_rdd = sc.parallelize(sample_seeds.to_dict('records')).repartition(100)\n",
    "print(\"sample_rdd partitions: {:d}\".format(sample_rdd.getNumPartitions()))\n",
    "\n",
    "# use mapPartitions() to now run each bootstrap sample in parallel\n",
    "results_df = sample_rdd.map(processASampleReturnSummary) \n",
    "\n",
    "bootstrap_results = pd.DataFrame(results_df.collect())\n",
    "print('completed creating pandas dataframe creation time: {}'.format(datetime.datetime.now() - bootstrap_start))\n",
    "print(bootstrap_results.shape)\n",
    "        \n",
    "print('shape of bootstrap_results is {}'.format(bootstrap_results.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show sample results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "name 'bootstrap_results' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'bootstrap_results' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bootstrap_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "name 'bootstrap_results' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'bootstrap_results' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bootstrap_results.tail()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
