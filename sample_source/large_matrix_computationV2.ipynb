{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1539969261381_0001</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-3-108.ec2.internal:20888/proxy/application_1539969261381_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-3-67.ec2.internal:8042/node/containerlogs/container_1539969261381_0001_01_000001/livy\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "<pyspark.sql.session.SparkSession object at 0x7fe9f5828d68>"
     ]
    }
   ],
   "source": [
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd, numpy as np\n",
    "import numpy as np\n",
    "from pyspark.mllib.linalg.distributed import BlockMatrix\n",
    "from pyspark.mllib.linalg import DenseMatrix, Matrices\n",
    "from pyspark.sql.functions import randn\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Generate random Matrix data and put in Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ROWS = int(1e6)\n",
    "NUM_COLS = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(13)\n",
    "seeds = np.random.choice(range(int(1e6)),NUM_COLS,replace=False).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sprk_df = spark.range(0,NUM_ROWS,numPartitions=100)\n",
    "\n",
    "for i in range(len(seeds)):\n",
    "    sprk_df = sprk_df.withColumn('X'+str(i+1),randn(seed=seeds[i]))\n",
    "    \n",
    "sprk_df = sprk_df.drop('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                  X1|                 X10|                X149|\n",
      "+--------------------+--------------------+--------------------+\n",
      "| -0.7875995423413541| -1.8496231710670317| 0.14319168635324628|\n",
      "| -0.8802204696629468|  1.0227987046234572|-0.16021299654078477|\n",
      "|-0.13052186437684288| -0.6377570013161057| 0.17379403751949127|\n",
      "| -0.9819029238098963| -1.3914036254270399|-0.07337543730763679|\n",
      "| -1.1528200936909183|  0.7931504380379168| -1.1181662576458717|\n",
      "|  1.4381143086517836| -0.5923779893720164| 0.46605793183654703|\n",
      "| -0.8443907079566332|-0.14327078654999217|  1.9005297473603961|\n",
      "|  1.3409502738035992|  0.4590974616850282| -1.1233419119648724|\n",
      "|  0.7712124278965381|  1.4994883973477136|   1.614813091130632|\n",
      "|  -0.631149997027938| -0.3391861350329733| -1.8154550466148764|\n",
      "+--------------------+--------------------+--------------------+"
     ]
    }
   ],
   "source": [
    "sprk_df.limit(10).select(['X1','X10','X149']).show(10)  #toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,000,000"
     ]
    }
   ],
   "source": [
    "print('{:,d}'.format(sprk_df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read matrix data from Spark Dataframe and perform X.T @ X matrix operation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "start_time = datetime.datetime.now()\n",
    "A = np.array(sprk_df.collect())\n",
    "\n",
    "print(A.shape)\n",
    "\n",
    "B = np.dot(A.T, A)\n",
    "\n",
    "print(B.shape)\n",
    "print('time for numpy: {:.3f}'.format((datetime.datetime.now() - start_time).total_seconds()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create BlockMatrix from Spark dataframe and perform X.T @ X matrix operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "rdd= sprk_df.rdd.map(list)\n",
    "COLUMNS = len(rdd.take(1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create closure for partitioning function to consolidate single row dense matrix into larger blocks\n",
    "# Based on number of dense matrices and specified rows per block, calculate number of blocks to make up the \n",
    "# block matrix and return a partition function to be used in partitionBy() function\n",
    "\n",
    "def createPartitionFunction(num_items,block_rows):\n",
    "    # Calculate number of blocks that will make up the BlockMatrix\n",
    "    number_of_blocks = int(np.ceil(num_items/block_rows))\n",
    "    \n",
    "    # return number of blocks and closure to be used by partitionBy() function\n",
    "    return number_of_blocks, lambda k:int(k/block_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS_PER_BLOCK = 1600\n",
    "number_of_blocks, partition_function = createPartitionFunction(rdd.count(),ROWS_PER_BLOCK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to stack single row dense matrices into larger block based on partitionBy()\n",
    "\n",
    "def stackDenseMatrices(iter_x):\n",
    "    # convert each dense matrix in this partition to an element in a list\n",
    "    vector_list = [dm[1].toArray() for dm in iter_x]\n",
    "    \n",
    "    # stack all the elements in the list to a single array\n",
    "    combined_array = np.vstack(vector_list)\n",
    "\n",
    "    # convert the single array to a larger dense matrix\n",
    "    values = [x for x in combined_array.flatten('F').tolist()]\n",
    "    den_mat = DenseMatrix(combined_array.shape[0],combined_array.shape[1],\n",
    "                          values)\n",
    "    \n",
    "    # return larger dense matrix\n",
    "    return [den_mat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert rdd of single row dense matrices to rdd of larger dense matrices to form the BlockMatrix\n",
    "\n",
    "# create key-value rdd with index value of single-row dense matrix as key\n",
    "rdd2 = rdd.map(lambda x: DenseMatrix(1,COLUMNS,x)).zipWithIndex().map(lambda x:(x[1],x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repartition rdd such that all single-row dense matrices that make up a block are in a single partition\n",
    "rdd2 = rdd2.partitionBy(number_of_blocks,partition_function)\n",
    "\n",
    "\n",
    "# Based on number of blocks, addd partition number to key and ensure within partition rows are in order\n",
    "def addPartitionIdToKey(x):\n",
    "    partition_id = x[1]\n",
    "    dm_list = [((partition_id,y[0]),y[1]) for y in x[0]]\n",
    "    return dm_list\n",
    "    \n",
    "rdd2 = rdd2.mapPartitions(lambda x:[list(x)]).zipWithIndex().map(addPartitionIdToKey).flatMap(lambda x:x)\n",
    "\n",
    "# create partitions by block and ensure rows are sorted correctly\n",
    "rdd2 =  rdd2.repartitionAndSortWithinPartitions(number_of_blocks,lambda k:k[0])\n",
    "\n",
    "\n",
    "# stack all single-row dense matrices in a partition into one larger dense matrix\n",
    "rdd2 = rdd2.mapPartitions(stackDenseMatrices)\n",
    "\n",
    "# create rdd of block matrix structure\n",
    "rdd2 = rdd2.zipWithIndex().map(lambda x:((x[1],0),x[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Block_A = BlockMatrix(rdd2,ROWS_PER_BLOCK,COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Block_B = Block_A.transpose().multiply(Block_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for BlockMatrix: 276.926"
     ]
    }
   ],
   "source": [
    "B2 = Block_B.toLocalMatrix().toArray()\n",
    "print('time for BlockMatrix: {:.3f}'.format((datetime.datetime.now() - start_time).total_seconds()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare BlockMatrix answer with Numpy Answer and calculate relative error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "name 'B' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'B' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.array_equal(B,B2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "name 'B' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'B' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.max(np.abs(B2-B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "name 'B' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'B' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.max(np.abs((B2-B)/B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
